---
layout:     post
title:      "YOLO解读"
subtitle:   " \"You Only Look Once:
Unified, Real-Time Object Detection\""
date:       2017-11-30 18:00:00+0800
author:     "Sundrops"
header-img: "img/home-bg-faye.png"
catalog: true
tags:
    - detection
---

> 上一篇讲了region-free结构的检测模型SSD，这次region-free的鼻祖[You Only Look Once:
Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)，这是它的第一个版本，它还有v2，以后再讲。

## 整体特点 ##

第一个颠覆ross的RCNN系列，提出region-free，把检测任务直接转换为回归来做，第一次做到精度可以，且实时性很好。
1. 直接将原图划分为SxS个grid cell，如果有物体的中心落到这个格子里那么这个格子的gt就是这个物体(如果有多个物体的中心落到这个格子里，就取距离格子中心点最近的那个物体为gt，我的猜想，需要看代码验证)。
2. 每个格子被指定的gt需要对应B个bounding box(下面检测bbox)去回归，也就是说每个格子对应的B个bbox的gt是一样的(我也很奇怪这一点为什么同一个物体需要两个bbox去回归，为了提高精度吧)
3. 每个bbox预测5个值: x, y, w, h, 置信度。(x, y)是bbox的中心在对应格子里的相对位置，范围[0,1]。(w, h)是bbox相对于全图的的长宽，范围[0,1]。x, y, w, h的4个gt值可以算出来。confidence = P(object)\* iou, 它的gt值是这样指定的: 若bbox对应格子包含物体，则P(object) = 1，否则P(object) = 0。
4. 每个格子也会预测属于各个类别的置信度，也就是每个格子对应的B个box是共享这个值的，这B个box只能属于一类的，所以和第一步呼应它们的gt都是一样的。
5. inference阶段![这里写图片描述](http://img.blog.csdn.net/20171130164440061?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzAxMDg4OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，class-specific confidence score既包含了bounding box最终属于哪个类别的概率，又包含了bounding box位置的准确度。最后设置一个阈值与class-specific confidence score对比，过滤掉score低于阈值的boxes，然后对score高于阈值的boxes进行非极大值抑制(NMS, non-maximum suppression)后得到最终的检测框体。

**其他示意图**

![这里写图片描述](http://img.blog.csdn.net/20171130151400339?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzAxMDg4OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20171130151414684?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzAxMDg4OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20171130164522955?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzAxMDg4OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 训练细节##

此处为转载，出处见结尾。
YOLO使用均方和误差作为loss函数来优化模型参数，即网络输出的S\*S\*(B\*5 + C)维向量与真实图像的对应S\*S\*(B\*5 + C)维向量的均方和误差。如下式所示。其中，coordError、iouError和classError分别代表预测数据与标定数据之间的坐标误差、IOU误差和分类误差。
每个格子的 **loss=coordError + iouError + classError**
YOLO对上式loss的计算进行了如下修正。

1. 位置相关误差（坐标、IOU）与分类误差对网络loss的贡献值是不同的，因此YOLO在计算loss时，使用λcoord =5修正coordError。
2.  在计算IOU误差时，包含物体的格子与不包含物体的格子，二者的IOU误差对网络loss的贡献值是不同的。若采用相同的权值，那么不包含物体的格子的confidence值近似为0，变相放大了包含物体的格子的confidence误差在计算网络参数梯度时的影响。为解决这个问题，YOLO 使用λnoobj =0.5修正iouError。（注此处的‘包含’是指存在一个物体，它的中心坐标落入到格子内）。
3. 对于相等的误差值，大物体误差对检测的影响应小于小物体误差对检测的影响。这是因为，相同的位置偏差占大物体的比例远小于同等偏差占小物体的比例。YOLO将物体大小的信息项（w和h）进行求平方根来改进这个问题。（注：这个方法并不能完全解决这个问题）。

综上，YOLO在训练过程中Loss计算如下式所示：

![这里写图片描述](http://img.blog.csdn.net/20171130165118483?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzAxMDg4OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 总结 ##
此处为转载，出处见结尾。
综上所述，YOLO有如下特点：

1. 快。YOLO将物体检测作为回归问题进行求解，整个检测网络pipeline简单，且训练只需一次完成。
2. 背景误检率低。YOLO在训练和推理过程中能“看到”整张图像的整体信息，而基于region proposal的物体检测方法（如Fast RCNN）在检测过程中，只“看到”候选框内的局部图像信息。因此，若当图像背景（非物体）中的部分数据被包含在候选框中送入检测网络进行检测时，容易被误检测成物体[1]。
3. 识别物体位置精准性差，√w和√h策略并没有完全解决location准确度问题。
4. 召回率低，尤其是对小目标(密集的小物体更差，它们可能同类可能不同类都在一个cell里，虽然一个cell对应B个bbox，但是它们共用一个gt，这就导致一些物体被忽略掉，且infer时B个bbox也只是输出一个综合置信度最高的bbox)。

----------
参考了
[YOLO的slides](https://docs.google.com/presentation/d/1kAa7NOamBt4calBU9iHgT8a86RRHz9Yz2oh4-GTdX6M/edit#slide=id.g15092aa245_0_230)
[YOLO详解：评论里很多对话很赞](https://zhuanlan.zhihu.com/p/25236464)
[还是之前喜欢的专栏: You Only Look Once: Unified, Real-Time Object Detection(YOLO)](https://zhuanlan.zhihu.com/p/31427164)
